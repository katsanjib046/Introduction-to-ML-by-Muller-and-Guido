{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_ConvNets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dhrFdArZO24x",
        "PJ-8QdosP0HE",
        "uoUCjgVbQeyj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiDG3S9Hsneb"
      },
      "source": [
        "# Simple convolutional network for MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_FCLGuhuQYP"
      },
      "source": [
        "Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs). The big event was <a href=\"https://en.wikipedia.org/wiki/AlexNet\">AlexNet</a> winning the <a href=\"https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge\">ImageNet Large Scale Visual Recognition Challenge</a>.\n",
        "\n",
        "<img src=\"https://cms.qz.com/wp-content/uploads/2017/07/image_classification_006.png?quality=75&strip=all&w=1240&h=1602&crop=1\" width=600>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dT2PA_bNsYGb",
        "outputId": "66128b19-1d71-49dd-8540-3874b5df155b"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nPDDBk2tNhI"
      },
      "source": [
        "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that we've already tackled several times, most recently with a feed-forward neural net with several dense layers (our test accuracy then was 97.8%). Even though our convnet here will be very basic, its accuracy will still blow out of the water that of the previous densely-connected model.\n",
        "\n",
        "The 6 lines of code below show you what a basic convnet looks like. It's a stack of Conv2D and MaxPooling2D layers. We'll see in a minute what they do concretely. Importantly, a convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension). In our case, we will configure our convnet to process inputs of size (28, 28, 1), which is the format of MNIST images (they are grayscale). We do this via passing the argument input_shape=(28, 28, 1) to our first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnqvCd_otUhu",
        "outputId": "32499e17-d8a6-4aca-db00-ae7520f43417"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkxngzQAv2oa"
      },
      "source": [
        "You can see above that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (e.g. 32 or 64).\n",
        "\n",
        "The next step would be to feed our last output tensor (of shape (3, 3, 64)) into a densely-connected classifier network like those you are already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D, and then add a few Dense layers on top:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ZMg4nyvnna"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKF4PQL5wnjK"
      },
      "source": [
        "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfY61KN4whS9",
        "outputId": "674d5092-e1ef-4599-b6bd-94f77d82d8ef"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G407VTxJw3aE"
      },
      "source": [
        "As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576,), before going through two Dense layers. \n",
        "\n",
        "In summary, this is what we did so far:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*5A4b1qOZIr4Q6SKceqGn7w.jpeg\" width=800>\n",
        "\n",
        "Now, let's train our convnet on the MNIST digits. We will reuse a lot of our previous code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6Ei5r2EwsUS",
        "outputId": "d50cbaa1-d17f-41bb-abf7-f2b5a374ecfc"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emLIWJ8jxKvz",
        "outputId": "58faeac5-3861-4a32-a078-5c5c53bcce21"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 52s 55ms/step - loss: 0.1656 - accuracy: 0.9473\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 52s 55ms/step - loss: 0.0476 - accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 51s 54ms/step - loss: 0.0324 - accuracy: 0.9900\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 51s 54ms/step - loss: 0.0256 - accuracy: 0.9919\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 52s 55ms/step - loss: 0.0202 - accuracy: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffac1f69710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMiCoZLxbvi"
      },
      "source": [
        "Let's evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1TQrbBqxRw4",
        "outputId": "0e11a81c-afc7-4c62-c7cf-6957080ed19c"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0256 - accuracy: 0.9920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5GoO4Ucxihm",
        "outputId": "e90ab06c-030e-485a-be2e-78fd548d4cf4"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9919999837875366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD8a02DXxoxw"
      },
      "source": [
        "While our previous densely-connected network from had a test accuracy of 97.8%, this basic convnet has a test accuracy of 99.2%: we decreased our error rate by about two-thirds (relative). Not bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bedOJrrJ1LFq"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3U22OdnFzPA"
      },
      "source": [
        "## Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNzkAQn2yRT"
      },
      "source": [
        "The fundamental difference between a dense layer and a convolution layer is this:\n",
        "\n",
        "* **Dense** layers learn global patterns in their feature space; in the case of a MNIST digit, patterns involving **all** pixels (recall the 3Blue1Brown videos).\n",
        "* **Convolutional** layers learn local patterns; in the case of a MNIST digit, patterns involving a small 2D window of pixels, here 3x3 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JlQLHHx6vfd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8MSeBicm6deh",
        "outputId": "d128df77-5192-4f7e-9c31-7280bf6d23ce"
      },
      "source": [
        "# show one training sample\n",
        "image = train_images[2]\n",
        "fig = plt.figure\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM5klEQVR4nO3db4hd9Z3H8c8n2oDYKom6w2CCZksUyhLtEmV1RbPEhmyexD6wNGjNsuIIVmhhH1TcBxVkQRfbZZ9YmKokXbOWQhwNpW6bDUW3oGEmktX8MYkbEjtDTCoiTVHsRr/7YE66Y5x77uTcc+65M9/3Cy733vO9594vh3zyO3/unZ8jQgAWvkVtNwCgPwg7kARhB5Ig7EAShB1I4sJ+fphtTv0DDYsIz7a8p5Hd9nrbh2y/bfuhXt4LQLNc9Tq77QskHZb0NUmTksYlbYqIAyXrMLIDDWtiZL9R0tsRcTQi/ijpp5I29vB+ABrUS9ivlPTbGc8ni2WfYXvE9oTtiR4+C0CPGj9BFxGjkkYlduOBNvUysk9JWj7j+bJiGYAB1EvYxyWttL3C9mJJ35S0o562ANSt8m58RJyx/aCkX0q6QNIzEbG/ts4A1KrypbdKH8YxO9C4Rr5UA2D+IOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJylM2A4Nu7dq1HWvbtm0rXfe2224rrR86dKhST23qKey2j0k6LekTSWciYnUdTQGoXx0j+99ExHs1vA+ABnHMDiTRa9hD0q9s77E9MtsLbI/YnrA90eNnAehBr7vxt0TElO0/k7TT9lsR8crMF0TEqKRRSbIdPX4egIp6GtkjYqq4PyVpTNKNdTQFoH6Vw277YttfOvtY0jpJ++pqDEC9etmNH5I0Zvvs+/x7RPxHLV014NZbby2tX3bZZaX1sbGxOttBH9xwww0da+Pj433sZDBUDntEHJV0XY29AGgQl96AJAg7kARhB5Ig7EAShB1IIs1PXNesWVNaX7lyZWmdS2+DZ9Gi8rFqxYoVHWtXXXVV6brFJeUFhZEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JIc539nnvuKa2/+uqrfeoEdRkeHi6t33fffR1rzz77bOm6b731VqWeBhkjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kkeY6e7ffPmP+eeqppyqve+TIkRo7mR9IAJAEYQeSIOxAEoQdSIKwA0kQdiAJwg4ksWCus69ataq0PjQ01KdO0C+XXnpp5XV37txZYyfzQ9eR3fYztk/Z3jdj2VLbO20fKe6XNNsmgF7NZTd+i6T15yx7SNKuiFgpaVfxHMAA6xr2iHhF0vvnLN4oaWvxeKukO2ruC0DNqh6zD0XEieLxu5I6HhDbHpE0UvFzANSk5xN0ERG2o6Q+KmlUkspeB6BZVS+9nbQ9LEnF/an6WgLQhKph3yFpc/F4s6QX62kHQFO67sbbfk7SGkmX256U9H1Jj0n6me17JR2X9I0mm5yLDRs2lNYvuuiiPnWCunT7bkTZ/OvdTE1NVV53vuoa9ojY1KG0tuZeADSIr8sCSRB2IAnCDiRB2IEkCDuQxIL5ieu1117b0/r79++vqRPU5Yknniitd7s0d/jw4Y6106dPV+ppPmNkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkFsx19l6Nj4+33cK8dMkll5TW168/92+V/r+77767dN1169ZV6umsRx99tGPtgw8+6Om95yNGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IguvshaVLl7b22dddd11p3XZp/fbbb+9YW7ZsWem6ixcvLq3fddddpfVFi8rHi48++qhjbffu3aXrfvzxx6X1Cy8s/+e7Z8+e0no2jOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjon8fZjf2YU8++WRp/f777y+td/t98zvvvHPePc3VqlWrSuvdrrOfOXOmY+3DDz8sXffAgQOl9W7XwicmJkrrL7/8csfayZMnS9ednJwsrS9ZsqS03u07BAtVRMz6D6bryG77GdunbO+bsewR21O29xa38snRAbRuLrvxWyTN9udG/iUiri9uv6i3LQB16xr2iHhF0vt96AVAg3o5Qfeg7TeK3fyOB0+2R2xP2C4/uAPQqKph/5GkL0u6XtIJST/o9MKIGI2I1RGxuuJnAahBpbBHxMmI+CQiPpX0Y0k31tsWgLpVCrvt4RlPvy5pX6fXAhgMXX/Pbvs5SWskXW57UtL3Ja2xfb2kkHRMUvlF7D544IEHSuvHjx8vrd988811tnNeul3Df+GFF0rrBw8e7Fh77bXXKvXUDyMjI6X1K664orR+9OjROttZ8LqGPSI2zbL46QZ6AdAgvi4LJEHYgSQIO5AEYQeSIOxAEmn+lPTjjz/edgs4x9q1a3taf/v27TV1kgMjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kkeY6OxaesbGxtluYVxjZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAl+z46BZbu0fs0115TWB3m66jZ0HdltL7f9a9sHbO+3/Z1i+VLbO20fKe6XNN8ugKrmsht/RtI/RMRXJP2VpG/b/oqkhyTtioiVknYVzwEMqK5hj4gTEfF68fi0pIOSrpS0UdLW4mVbJd3RVJMAendex+y2r5b0VUm7JQ1FxImi9K6koQ7rjEgaqd4igDrM+Wy87S9K2i7puxHx+5m1iAhJMdt6ETEaEasjYnVPnQLoyZzCbvsLmg76toh4vlh80vZwUR+WdKqZFgHUYS5n4y3paUkHI+KHM0o7JG0uHm+W9GL97SGziCi9LVq0qPSGz5rLMftfS/qWpDdt7y2WPSzpMUk/s32vpOOSvtFMiwDq0DXsEfEbSZ2+3bC23nYANIV9HSAJwg4kQdiBJAg7kARhB5LgJ66Yt2666abS+pYtW/rTyDzByA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCdHQOr25+SxvlhZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLjOjta89NJLpfU777yzT53kwMgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0k4IspfYC+X9BNJQ5JC0mhE/KvtRyTdJ+l3xUsfjohfdHmv8g8D0LOImPUPAcwl7MOShiPiddtfkrRH0h2ano/9DxHxxFybIOxA8zqFfS7zs5+QdKJ4fNr2QUlX1tsegKad1zG77aslfVXS7mLRg7bfsP2M7SUd1hmxPWF7oqdOAfSk6278n15of1HSy5L+KSKetz0k6T1NH8c/quld/b/v8h7sxgMNq3zMLkm2vyDp55J+GRE/nKV+taSfR8RfdHkfwg40rFPYu+7Ge/pPfD4t6eDMoBcn7s76uqR9vTYJoDlzORt/i6T/kvSmpE+LxQ9L2iTpek3vxh+TdH9xMq/svRjZgYb1tBtfF8IONK/ybjyAhYGwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRL+nbH5P0vEZzy8vlg2iQe1tUPuS6K2qOnu7qlOhr79n/9yH2xMRsbq1BkoMam+D2pdEb1X1qzd244EkCDuQRNthH23588sMam+D2pdEb1X1pbdWj9kB9E/bIzuAPiHsQBKthN32etuHbL9t+6E2eujE9jHbb9re2/b8dMUceqds75uxbKntnbaPFPezzrHXUm+P2J4qtt1e2xta6m257V/bPmB7v+3vFMtb3XYlffVlu/X9mN32BZIOS/qapElJ45I2RcSBvjbSge1jklZHROtfwLB9q6Q/SPrJ2am1bP+zpPcj4rHiP8olEfG9AentEZ3nNN4N9dZpmvG/U4vbrs7pz6toY2S/UdLbEXE0Iv4o6aeSNrbQx8CLiFckvX/O4o2SthaPt2r6H0vfdehtIETEiYh4vXh8WtLZacZb3XYlffVFG2G/UtJvZzyf1GDN9x6SfmV7j+2RtpuZxdCMabbelTTUZjOz6DqNdz+dM834wGy7KtOf94oTdJ93S0T8paS/lfTtYnd1IMX0MdggXTv9kaQva3oOwBOSftBmM8U049slfTcifj+z1ua2m6Wvvmy3NsI+JWn5jOfLimUDISKmivtTksY0fdgxSE6enUG3uD/Vcj9/EhEnI+KTiPhU0o/V4rYrphnfLmlbRDxfLG59283WV7+2WxthH5e00vYK24slfVPSjhb6+BzbFxcnTmT7YknrNHhTUe+QtLl4vFnSiy328hmDMo13p2nG1fK2a33684jo+03SBk2fkf8fSf/YRg8d+vpzSf9d3Pa33Zuk5zS9W/e/mj63ca+kyyTtknRE0n9KWjpAvf2bpqf2fkPTwRpuqbdbNL2L/oakvcVtQ9vbrqSvvmw3vi4LJMEJOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1I4v8A42HwKD7hFIAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcS9QUp38cIf"
      },
      "source": [
        "As a result, convolutional networks have two interesting properties:\n",
        "\n",
        "* **The learned patterns are translation invariant**. Once they learn a pattern appearing in a certain location, e.g., a vertical line, they can recognize it anywhere on the picture. This makes them very efficient for images -- they need fewer training samples in order to learn representations which can generalize easily.\n",
        "* **They can learn spacial hierarchies of patterns**. The first convolution layer would learn small local patterns such as edges and their shape and orientation (e.g., circle, vertical line, horizontal line). The next will learn larger patterns made of the features learned by the first layer, e.g., cat eye = circle + short vertical line; cat whisker = long horizontal line, cat nose = inverted triangle, etc. The third layer will learn to assemble these into the image of a cat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb3f_j3JAhIT"
      },
      "source": [
        "Convolution layers operate on 3D tensors, called **feature maps**, with two spatial axes (**height** and **width**) and a **depth** (or **channel**) axis.\n",
        "\n",
        "* For a **black-and-white image** like the MNIST digits, the depth is 1 (level of gray). \n",
        "* For a **color image**, the depth is 3, because the image has 3 color channels (red, green, blue).\n",
        "* The convolution operation extracts patches from the input feature map, applies the same transformation, and produces an **output feature map**, which is still a 3D tensor with height, width, and depth, but now the depth is a parameter of the layer which indicates the number of applied **filters**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1skUidH_GSvH"
      },
      "source": [
        "## The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZs9e4hImai"
      },
      "source": [
        "The convolution operation requires an **input dataset** and a convolution **filter** (**kernel**).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOzMMqOBJJdI"
      },
      "source": [
        "Now place the $n\\times n$ filter at each possible location within the $N\\times N$ data and compute the tensor product, storing the result in a new array of dimension $(N-n+1)\\times (N-n+1)$:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*ghaknijNGolaA3DpjvDxfQ@2x.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88abeI3JKD4X"
      },
      "source": [
        "Here is an animation how this works:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQmsI-gXNK8m"
      },
      "source": [
        "For color images of shape $(N, N, 3)$ the filter also has a shape of $(n,n,3)$ so that the result of the convolution is still a scalar.\n",
        "\n",
        "We can now apply a second filter, a third filter, etc. The number of filters is the **output depth** parameter, which is specified by the first argument in layers.Conv2D. An illustration for two different filters:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*45GSvnTvpHV0oiRr78dBiw@2x.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhrFdArZO24x"
      },
      "source": [
        "## Border effects, padding and stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ-8QdosP0HE"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFjOqFoPH3q"
      },
      "source": [
        "For any filter at least as large as $2\\times 2$, the output layer will be smaller than the input layer, which is due to border effects. This can be avoided by adding **padding** outside the input data frame:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*W2D564Gkad9lj3_6t9I2PA@2x.gif\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoUCjgVbQeyj"
      },
      "source": [
        "### Stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRSXbhRTQg_B"
      },
      "source": [
        "The stride determines how quickly we move the filter. The default is 1, but we can change it, e.g., to 2:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*4wZt9G7W7CchZO-5rVxl5g@2x.gif\" width=700>\n",
        "\n",
        "Notice how the output feature map got smaller.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59NExWM2RTec"
      },
      "source": [
        "## The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kwwQxGpRbF-"
      },
      "source": [
        "Max pooling downsamples the feature maps, reducing their size (similar to **strides**). Max pooling extracts windows from the input feature maps and computes the maximum value found in each one. For example:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP6icOq4SpmO"
      },
      "source": [
        "Max pooling is typically done with a $2\\times 2$ window, in which case the effect is similar to a convolution with stride 2 and a kernel which is simply the max function.\n",
        "\n",
        "Max pooling has two goals:\n",
        "\n",
        "* To allow subsequent layers look at much larger regions of the input data, allowing to learn high level patterns more easily.\n",
        "* To reduce the number of tunable parameters and thus help with overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW5DrE1YUgN2"
      },
      "source": [
        "Max pooling is better at downsampling than strides because it is more informative to look at the **maximal** presence of different features than the **average** presence of those features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gefNzHGGVL7O"
      },
      "source": [
        "Further reading and figure credits: <a href=\"https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\">Applied Deep Learning - Part 4: Convolutional Neural Networks</a>"
      ]
    }
  ]
}